<?xml version="1.0" encoding="ISO-8859-1"?>
<html xmlns="http://www.w3.org/1999/xhtml"><head xmlns="http://www.w3.org/1999/xhtml"><title xmlns="http://www.w3.org/1999/xhtml">Notes</title><meta xmlns="http://www.w3.org/1999/xhtml" name="generator" content="DocBook XSL Stylesheets V1.49"/><link rel="home" href="index.html" title="The Cathedral and the Bazaar"/><link rel="up" href="index.html" title="The Cathedral and the Bazaar"/><link rel="previous" href="ar01s13.html" title="Epilog: Netscape Embraces the Bazaar"/><link rel="next" href="ar01s15.html" title="Bibliography"/></head><body><div xmlns="http://www.w3.org/1999/xhtml" class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">Notes</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="ar01s13.html">Prev</a> </td><th width="60%" align="center"> </th><td width="20%" align="right"> <a accesskey="n" href="ar01s15.html">Next</a></td></tr></table><hr/></div><div xmlns="http://www.w3.org/1999/xhtml" class="sect1"><div xmlns="http://www.w3.org/1999/xhtml" class="titlepage"><div xmlns="http://www.w3.org/1999/xhtml"><h2 class="title" style="clear: both"><a xmlns="http://www.w3.org/1999/xhtml" id="id2826510"/>Notes</h2></div></div><p xmlns="http://www.w3.org/1999/xhtml"><i xmlns="http://www.w3.org/1999/xhtml"><a xmlns="http://www.w3.org/1999/xhtml" id="JB"/>[JB]</i> In
<i xmlns="http://www.w3.org/1999/xhtml">Programing Pearls</i>, the noted computer-science
aphorist Jon Bentley comments on Brooks's observation with ``If you
plan to throw one away, you will throw away two.''.  He is almost
certainly right.  The point of Brooks's observation, and Bentley's,
isn't merely that you should expect first attempt to be wrong, it's
that starting over with the right idea is usually more effective than
trying to salvage a mess.</p><p xmlns="http://www.w3.org/1999/xhtml"><span xmlns="http://www.w3.org/1999/xhtml" class="emphasis"><i xmlns="http://www.w3.org/1999/xhtml"><a xmlns="http://www.w3.org/1999/xhtml" id="QR"/>[QR]</i></span> Examples of successful
open-source, bazaar development predating the Internet explosion and
unrelated to the Unix and Internet traditions have existed.  The
development of the
<a xmlns="http://www.w3.org/1999/xhtml" href="http://www.cdrom.com/pub/infozip/" target="_top">info-Zip</a>
compression utility during 1990&#8211;x1992, primarily for DOS machines, was
one such example.  Another was the RBBS bulletin board system (again for DOS),
which began in 1983 and developed a sufficiently strong community that
there have been fairly regular releases up to the present (mid-1999)
despite the huge technical advantages of Internet mail and
file-sharing over local BBSs.  While the info-Zip community relied to
some extent on Internet mail, the RBBS developer culture was actually
able to base a substantial on-line community on RBBS that was
completely independent of the TCP/IP infrastructure.</p><p xmlns="http://www.w3.org/1999/xhtml"><span xmlns="http://www.w3.org/1999/xhtml" class="emphasis"><i xmlns="http://www.w3.org/1999/xhtml"><a xmlns="http://www.w3.org/1999/xhtml" id="CV"/>[CV]</i></span> That transparency and
peer review are valuable for taming the complexity of OS development
turns out, after all, not to be a new concept.  In 1965, very early
in the history of time-sharing operating systems,
Corbató and Vyssotsky, co-designers of the Multics operating
system, <a xmlns="http://www.w3.org/1999/xhtml" href="http://www.multicians.org/fjcc1.html" target="_top">wrote</a></p><blockquote xmlns="http://www.w3.org/1999/xhtml" class="blockquote"><p xmlns="http://www.w3.org/1999/xhtml">It is expected that the Multics system will be
published when it is operating substantially... Such publication is
desirable for two reasons: First, the system should withstand public
scrutiny and criticism volunteered by interested readers; second, in
an age of increasing complexity, it is an obligation to present and
future system designers to make the inner operating system as lucid as
possible so as to reveal the basic system issues.</p></blockquote><p xmlns="http://www.w3.org/1999/xhtml"><span xmlns="http://www.w3.org/1999/xhtml" class="emphasis"><i xmlns="http://www.w3.org/1999/xhtml"><a xmlns="http://www.w3.org/1999/xhtml" id="JH"/>[JH]</i></span> John Hasler has suggested
an interesting explanation for the fact that duplication of effort
doesn't seem to be a net drag on open-source development. He proposes
what I'll dub ``Hasler's Law'': the costs of duplicated work tend to
scale sub-qadratically with team size&#8212;that is, more slowly than the
planning and management overhead that would be needed to eliminate
them.</p><p xmlns="http://www.w3.org/1999/xhtml">This claim actually does not contradict Brooks's Law.  It may be
the case that total complexity overhead and vulnerability to bugs
scales with the square of team size, but that the costs from
<span xmlns="http://www.w3.org/1999/xhtml" class="emphasis"><i xmlns="http://www.w3.org/1999/xhtml">duplicated</i></span> work are nevertheless a special case
that scales more slowly.  It's not hard to develop plausible reasons
for this, starting with the undoubted fact that it is much easier to
agree on functional boundaries between different developers' code that
will prevent duplication of effort than it is to prevent the kinds of
unplanned bad interactions across the whole system that underly most
bugs.</p><p xmlns="http://www.w3.org/1999/xhtml">The combination of Linus's Law and Hasler's Law suggests that there
are actually three critical size regimes in software projects.  On
small projects (I would say one to at most three developers) no
management structure more elaborate than picking a lead programmer is
needed.  And there is some intermediate range above that in which the
cost of traditional management is relatively low, so its benefits from
avoiding duplication of effort, bug-tracking, and pushing to see that
details are not overlooked actually net out positive.</p><p xmlns="http://www.w3.org/1999/xhtml">Above that, however, the combination of Linus's Law and Hasler's Law
suggests there is a large-project range in which the costs and
problems of traditional management rise much faster than the expected
cost from duplication of effort.  Not the least of these costs is a
structural inability to harness the many-eyeballs effect, which (as
we've seen) seems to do a much better job than traditional management
at making sure bugs and details are not overlooked.  Thus, in the
large-project case, the combination of these laws effectively drives
the net payoff of traditional management to zero.</p><p xmlns="http://www.w3.org/1999/xhtml"><span xmlns="http://www.w3.org/1999/xhtml" class="emphasis"><i xmlns="http://www.w3.org/1999/xhtml"><a xmlns="http://www.w3.org/1999/xhtml" id="HBS"/>[HBS]</i></span> The split between Linux's
experimental and stable versions has another function related to, but
distinct from, hedging risk.  The split attacks another problem: the
deadliness of deadlines.  When programmers are held both to an
immutable feature list and a fixed drop-dead date, quality goes out
the window and there is likely a colossal mess in the making. I am
indebted to Marco Iansiti and Alan MacCormack of the Harvard Business
School for showing me me evidence that relaxing either one of these
constraints can make scheduling workable.</p><p xmlns="http://www.w3.org/1999/xhtml">One way to do this is to fix the deadline but leave the feature
list flexible, allowing features to drop off if not completed by
deadline.  This is essentially the strategy of the &quot;stable&quot; kernel
branch; Alan Cox (the stable-kernel maintainer) puts out releases at
fairly regular intervals, but makes no guarantees about when
particular bugs will be fixed or what features will beback-ported from
the experimental branch.</p><p xmlns="http://www.w3.org/1999/xhtml">The other way to do this is to set a desired feature list and deliver
only when it is done.  This is essentially the strategy of the
&quot;experimental&quot; kernel branch.  De Marco and Lister cited research
showing that this scheduling policy (&quot;wake me up when it's done&quot;)
produces not only the highest quality but, on average, shorter
delivery times than either &quot;realistic&quot; or &quot;aggressive&quot; scheduling.</p><p xmlns="http://www.w3.org/1999/xhtml">I have come to suspect (as of early 2000) that in earlier
versions of this essay I severely underestimated the importance of the
&quot;wake me up when it's done&quot; anti-deadline policy to the open-source
community's productivity and quality.  General experience with the
rushed GNOME 1.0 release in 1999 suggests that pressure for a
premature release can neutralize many of the quality benefits open
source normally confers.</p><p xmlns="http://www.w3.org/1999/xhtml">It may well turn out to be that the process transparency of open
source is one of three co-equal drivers of its quality, along with
&quot;wake me up when it's done&quot; scheduling and developer self-selection.</p><p xmlns="http://www.w3.org/1999/xhtml"><span xmlns="http://www.w3.org/1999/xhtml" class="emphasis"><i xmlns="http://www.w3.org/1999/xhtml"><a xmlns="http://www.w3.org/1999/xhtml" id="SU"/>[SU]</i></span> It's tempting, and not
entirely inaccurate, to see the core-plus-halo organization
characteristic of open-source projects as an Internet-enabled spin on
Brooks's own recommendation for solving the N-squared complexity
problem, the &quot;surgical-team&quot; organization&#8212;but the differences are
significant. The constellation of specialist roles such as &quot;code
librarian&quot; that Brooks envisioned around the team leader doesn't
really exist; those roles are executed instead by generalists aided by
toolsets quite a bit more powerful than those of Brooks's day.  Also, the
open-source culture leans heavily on strong Unix traditions of
modularity, APIs, and information hiding&#8212;none of which were
elements of Brooks's prescription.</p><p xmlns="http://www.w3.org/1999/xhtml"><span xmlns="http://www.w3.org/1999/xhtml" class="emphasis"><i xmlns="http://www.w3.org/1999/xhtml"><a xmlns="http://www.w3.org/1999/xhtml" id="RJ"/>[RJ]</i></span> The respondent who
pointed out to me the effect of widely varying trace path lengths on
the difficulty of characterizing a bug speculated that trace-path
difficulty for multiple symptoms of the same bug varies
&quot;exponentially&quot; (which I take to mean on a Gaussian or Poisson
distribution, and agree seems very plausible).  If it is
experimentally possible to get a handle on the shape of this
distribution, that would be extremely valuable data.  Large departures
from a flat equal-probability distribution of trace difficulty would
suggest that even solo developers should emulate the bazaar strategy
by bounding the time they spend on tracing a given symptom before they
switch to another.  Persistence may not always be a virtue...</p><p xmlns="http://www.w3.org/1999/xhtml"><span xmlns="http://www.w3.org/1999/xhtml" class="emphasis"><i xmlns="http://www.w3.org/1999/xhtml"><a xmlns="http://www.w3.org/1999/xhtml" id="IN"/>[IN]</i></span> An issue related to
whether one can start projects from zero in the bazaar style is
whether the bazaar style is capable of supporting truly innovative
work.  Some claim that, lacking strong leadership, the bazaar can only
handle the cloning and improvement of ideas already present at the
engineering state of the art, but is unable to push the state of the
art.  This argument was perhaps most infamously made by the <a xmlns="http://www.w3.org/1999/xhtml" href="http://www.opensource.org/halloween/" target="_top">Halloween
Documents</a>, two embarrassing internal Microsoft memoranda
written about the open-source phenomenon.  The authors compared
Linux's development of a Unix-like operating system to ``chasing
taillights'', and opined ``(once a project has achieved &quot;parity&quot; with
the state-of-the-art), the level of management necessary to push
towards new frontiers becomes massive.''</p><p xmlns="http://www.w3.org/1999/xhtml">There are serious errors of fact implied in this argument.  One is
exposed when the Halloween authors themseselves later observe that
``often [...] new research ideas are first implemented and available
on Linux before they are available / incorporated into other
platforms.''</p><p xmlns="http://www.w3.org/1999/xhtml">If we read ``open source'' for ``Linux'', we see that this is far from
a new phenomenon.  Historically, the open-source community did not
invent Emacs or the World Wide Web or the Internet itself by chasing
taillights or being massively managed&#8212;and in the present, there
is so much innovative work going on in open source that one is spoiled
for choice.  The GNOME project (to pick one of many) is pushing the
state of the art in GUIs and object technology hard enough to have
attracted considerable notice in the computer trade press well outside
the Linux community.  Other examples are legion, as a visit to <a xmlns="http://www.w3.org/1999/xhtml" href="http://freshmeat.net/" target="_top">Freshmeat</a> on any given day will
quickly prove.</p><p xmlns="http://www.w3.org/1999/xhtml">But there is a more fundamental error in the implicit assumption
that the <span xmlns="http://www.w3.org/1999/xhtml" class="emphasis"><i xmlns="http://www.w3.org/1999/xhtml">cathedral model</i></span> (or the bazaar model, or
any other kind of management structure) can somehow make innovation
happen reliably.  This is nonsense.  Gangs don't have breakthrough
insights&#8212;even volunteer groups of bazaar anarchists are usually
incapable of genuine originality, let alone corporate committees of
people with a survival stake in some status quo ante.
<span xmlns="http://www.w3.org/1999/xhtml" class="emphasis"><i xmlns="http://www.w3.org/1999/xhtml">Insight comes from individuals.</i></span> The most their
surrounding social machinery can ever hope to do is to be
<span xmlns="http://www.w3.org/1999/xhtml" class="emphasis"><i xmlns="http://www.w3.org/1999/xhtml">responsive</i></span> to breakthrough insights&#8212;to nourish
and reward and rigorously test them instead of squashing them.</p><p xmlns="http://www.w3.org/1999/xhtml">Some will characterize this as a romantic view, a reversion to
outmoded lone-inventor stereotypes.  Not so; I am not asserting that
groups are incapable of <span xmlns="http://www.w3.org/1999/xhtml" class="emphasis"><i xmlns="http://www.w3.org/1999/xhtml">developing</i></span> breakthrough
insights once they have been hatched; indeed, we learn from the
peer-review process that such development groups are essential to
producing a high-quality result.  Rather I am pointing out that every
such group development starts from&#8212;is necessarily sparked by&#8212;one
good idea in one person's head.  Cathedrals and bazaars and other
social structures can catch that lightning and refine it, but they
cannot make it on demand.</p><p xmlns="http://www.w3.org/1999/xhtml">Therefore the root problem of innovation (in software, or anywhere
else) is indeed how not to squash it&#8212;but, even more fundamentally,
it is <span xmlns="http://www.w3.org/1999/xhtml" class="emphasis"><i xmlns="http://www.w3.org/1999/xhtml">how to grow lots of people who can have insights in the 
first place</i></span>.</p><p xmlns="http://www.w3.org/1999/xhtml">To suppose that cathedral-style development could manage this
trick but the low entry barriers and process fluidity of the bazaar
cannot would be absurd.  If what it takes is one person with one good
idea, then a social milieu in which one person can rapidly attract the
cooperation of hundreds or thousands of others with that good idea is
going inevitably to out-innovate any in which the person has to do a
political sales job to a hierarchy before he can work on his idea
without risk of getting fired.</p><p xmlns="http://www.w3.org/1999/xhtml">And, indeed, if we look at the history of software innovation by
organizations using the cathedral model, we quickly find it is rather
rare.  Large corporations rely on university research for new ideas
(thus the Halloween Documents authors' unease about Linux's facility
at coopting that research more rapidly).  Or they buy out small
companies built around some innovator's brain.  In neither case is the
innovation native to the cathedral culture; indeed, many innovations
so imported end up being quietly suffocated under the &quot;massive level of
management&quot; the Halloween Documents' authors so extol.</p><p xmlns="http://www.w3.org/1999/xhtml">That, however, is a negative point.  The reader would be better served
by a positive one.  I suggest, as an experiment, the following:</p><div xmlns="http://www.w3.org/1999/xhtml" class="itemizedlist"><ul type="disc"><li xmlns="http://www.w3.org/1999/xhtml"><p xmlns="http://www.w3.org/1999/xhtml"> Pick a criterion for originality that you believe you
       can apply consistently.  If your definition is ``I know it when
       I see it'', that's not a problem for purposes of this
       test.</p></li><li xmlns="http://www.w3.org/1999/xhtml"><p xmlns="http://www.w3.org/1999/xhtml">Pick any closed-source operating system competing with Linux,
       and a best source for accounts of current development work on
       it.</p></li><li xmlns="http://www.w3.org/1999/xhtml"><p xmlns="http://www.w3.org/1999/xhtml">Watch that source and Freshmeat for one month.  Every
       day, count the number of release announcements on Freshmeat
       that you consider `original' work.  Apply the same definition
       of `original' to announcements for that other OS and count
       them.</p></li><li xmlns="http://www.w3.org/1999/xhtml"><p xmlns="http://www.w3.org/1999/xhtml">Thirty days later, total up both figures.</p></li></ul></div><p xmlns="http://www.w3.org/1999/xhtml">The day I wrote this, Freshmeat carried twenty-two release
announcements, of which three appear they might push state of the art
in some respect, This was a slow day for Freshmeat, but I will be
astonished if any reader reports as many as three likely innovations
<span xmlns="http://www.w3.org/1999/xhtml" class="emphasis"><i xmlns="http://www.w3.org/1999/xhtml">a month</i></span> in any closed-source channel.</p><p xmlns="http://www.w3.org/1999/xhtml"><span xmlns="http://www.w3.org/1999/xhtml" class="emphasis"><i xmlns="http://www.w3.org/1999/xhtml"><a xmlns="http://www.w3.org/1999/xhtml" id="EGCS"/>[EGCS]</i></span> We now have
history on a project that, in several ways, may provide a more
indicative test of the bazaar premise than fetchmail; <a xmlns="http://www.w3.org/1999/xhtml" href="http://egcs.cygnus.com/" target="_top">EGCS</a>, the Experimental GNU
Compiler System.</p><p xmlns="http://www.w3.org/1999/xhtml">This project was announced in mid-August of 1997 as a conscious
attempt to apply the ideas in the early public versions of
<i xmlns="http://www.w3.org/1999/xhtml">The Cathedral and the Bazaar</i>. The project
founders felt that the development of GCC, the Gnu C Compiler, had
been stagnating.  For about twenty months afterwards, GCC and EGCS
continued as parallel products&#8212;both drawing from the same
Internet developer population, both starting from the same GCC source
base, both using pretty much the same Unix toolsets and development
environment.  The projects differed only in that EGCS consciously
tried to apply the bazaar tactics I have previously described, while
GCC retained a more cathedral-like organization with a closed
developer group and infrequent releases.</p><p xmlns="http://www.w3.org/1999/xhtml">This was about as close to a controlled experiment as one could ask
for, and the results were dramatic.  Within months, the EGCS versions
had pulled substantially ahead in features; better optimization,
better support for FORTRAN and C++.  Many people found the EGCS 
development snapshots to be more reliable than the most recent
stable version of GCC, and major Linux distributions began to
switch to EGCS.</p><p xmlns="http://www.w3.org/1999/xhtml">In April of 1999, the Free Software Foundation (the official
sponsors of GCC) dissolved the original GCC development group and
officially handed control of the project to the the EGCS steering
team.</p><p xmlns="http://www.w3.org/1999/xhtml"><span xmlns="http://www.w3.org/1999/xhtml" class="emphasis"><i xmlns="http://www.w3.org/1999/xhtml"><a xmlns="http://www.w3.org/1999/xhtml" id="SP"/>[SP]</i></span> Of course,
Kropotkin's critique and Linus's Law raise some wider issues about the
cybernetics of social organizations.  Another folk theorem of software
engineering suggests one of them; Conway's Law&#8212;commonly stated as
``If you have four groups working on a compiler, you'll get a 4-pass
compiler''.  The original statement was more general: ``Organizations
which design systems are constrained to produce designs which are
copies of the communication structures of these organizations.''  We
might put it more succinctly as ``The means determine the ends'', or
even ``Process becomes product''.</p><p xmlns="http://www.w3.org/1999/xhtml">It is accordingly worth noting that in the open-source community
organizational form and function match on many levels. The network is
everything and everywhere: not just the Internet, but the people doing
the work form a distributed, loosely coupled, peer-to-peer network
that provides multiple redundancy and degrades very gracefully.
In both networks, each node is important only to the extent that other
nodes want to cooperate with it.</p><p xmlns="http://www.w3.org/1999/xhtml">The peer-to-peer part is essential to the community's astonishing
productivity. The point Kropotkin was trying to make about power
relationships is developed further by the `SNAFU Principle': ``True
communication is possible only between equals, because inferiors are
more consistently rewarded for telling their superiors pleasant lies
than for telling the truth.''  Creative teamwork utterly depends on
true communication and is thus very seriously hindered by the presence
of power relationships.  The open-source community, effectively free
of such power relationships, is teaching us by contrast how dreadfully
much they cost in bugs, in lowered productivity, and in lost
opportunities.</p><p xmlns="http://www.w3.org/1999/xhtml">Further, the SNAFU principle predicts in authoritarian organizations 
a progressive disconnect between decision-makers and reality, as more
and more of the input to those who decide tends to become pleasant
lies.  The way this plays out in conventional software development
is easy to see; there are strong incentives for the inferiors to
hide, ignore, and minimize problems.  When this process becomes
product, software is a disaster.</p></div><div xmlns="http://www.w3.org/1999/xhtml" class="navfooter"><hr/><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="ar01s13.html">Prev</a> </td><td width="20%" align="center"><a accesskey="u" href="index.html">Up</a></td><td width="40%" align="right"> <a accesskey="n" href="ar01s15.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">Epilog: Netscape Embraces the Bazaar </td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top"> Bibliography</td></tr></table></div></body></html>
